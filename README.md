# Sequence-to-Sequence-Model-for-Machine-Translation
For this project, I compared two machine translation models: Traditional Sequence-to-Sequence Encoder-Decoder and Attention Mechanism. The focus was on evaluating their performance in handling long sentences. The findings indicate that the Attention Mechanism model outperforms the Traditional model in accurately translating lengthy sentences.

The Attention Mechanism model demonstrated a remarkable ability to handle the intricacies of long sentences. The higher average BLEU score of 25.21, compared to the baseline model's score of 19.48, further emphasised the improved translation quality achieved by this model. While the research provided promising results, I know its problems and limitations. The availability of training data, the selection of hyperparameters, and the computational resources all had a role in the models' performance. Furthermore, both models were sensitive to sentence length, indicating that more research into dealing with extremely long phrases is necessary. The Attention Mechanism model's higher performance has important implications for real-world machine translation applications. The ability of the attention mechanism to focus on relevant segments improves translation accuracy in fields where keeping the semantic and syntactic structure of long phrases is critical. Because of this discovery, the Attention Mechanism model is an appealing option for circumstances such as legal document translation, medical reports, and technical literature.
