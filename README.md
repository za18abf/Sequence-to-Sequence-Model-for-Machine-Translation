# Sequence-to-Sequence-Model-for-Machine-Translation
In my research, I compared two machine translation models: Traditional Sequence-to-Sequence Encoder-Decoder and Attention Mechanism. The focus was on evaluating their performance in handling long sentences. The findings indicate that the Attention Mechanism model outperforms the Traditional model in accurately translating lengthy sentences.

The Attention Mechanism model demonstrated a remarkable ability to handle the intricacies of long sentences. The higher average BLEU score of 25.21, compared to the baseline model's score of 19.48, further emphasised the improved translation quality achieved by this model. While the research provided promising results, I know its problems and limitations. The availability of training data, the selection of hyperparameters, and the computational resources all had a role in the models' performance. Furthermore, both models were sensitive to sentence length, indicating that more research into dealing with extremely long phrases is necessary.
